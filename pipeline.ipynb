{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d55efea-0659-4d7a-92e3-4db3efdd2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 16 10:50:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 510.60.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     On   | 00000000:00:06.0 Off |                  Off |\n",
      "| N/A   31C    P0    49W / 250W |      3MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 6000     On   | 00000000:00:07.0 Off |                  Off |\n",
      "| N/A   53C    P0   220W / 250W |  24166MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     30506      C   ./executable                    24163MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663a09ce-9213-4279-9c1a-6f773976f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hahajjjun/anaconda3/envs/molly/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "model = AutoModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33585e3-944e-43aa-9a0d-83c6f666ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5bb2d3-3e96-407e-a30f-ce38a8777dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    #TRAIN = os.path.join(path, 'train.csv')\n",
    "    TRAIN = os.path.join(path, 'train_data.csv')\n",
    "    VALID = os.path.join(path, 'val_data.csv')\n",
    "    TEST = os.path.join(path, 'test.csv')\n",
    "    SS = os.path.join(path, 'sample_submission.csv')\n",
    "    train = pd.read_csv(TRAIN)\n",
    "    valid = pd.read_csv(VALID)\n",
    "    test = pd.read_csv(TEST)\n",
    "    sample_submission = pd.read_csv(SS)\n",
    "    return train,valid,test,sample_submission\n",
    "    #return train, test, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c560f9d-f05f-4810-8dcf-4e638b0de27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test, ss = load_data('/home/hahajjjun/Junha Park/DACON_PYNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6c4c18-0de0-44b9-997a-260506b8d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(df):\n",
    "    df[\"text_sum\"] = \"[CLS] \"+df[\"code1\"]+\" [SEP] \"+df[\"code2\"]+\" [SEP]\"\n",
    "    df = df[['text_sum','similar']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5749a8a1-b114-4d57-a538-f712ae36d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = text_clean(train)\n",
    "valid = text_clean(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42451c2-c2cf-4576-8aab-ca4feb23fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codebert_transform(text):\n",
    "    transform = tokenizer(text,pad_to_max_length=True,truncation=True,max_length=512, return_tensors='pt',add_special_tokens=False)\n",
    "    return transform\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self,dataset,mode='train',transform=codebert_transform):\n",
    "        super(customDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset['text_sum'].iloc[idx]\n",
    "        tokens = self.transform(text)\n",
    "        token_ids = tokens['input_ids'][0]\n",
    "        attn_masks = tokens['attention_mask'][0]\n",
    "        #token_type_ids = tokens['token_type_ids'][0]\n",
    "\n",
    "        if self.mode == 'test':\n",
    "          return token_ids,attn_masks #,token_type_ids\n",
    "        else: \n",
    "          labels = self.dataset['similar'].iloc[idx]\n",
    "          return token_ids,attn_masks,labels #token_type_ids, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b1748d-c9b2-462f-bdff-a7e792e5062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    class codeBERTclassifier(nn.Module):\n",
    "        def __init__(self, model, hidden_size = 768, num_classes=2, params=None,  freeze=False):\n",
    "            super(codeBERTclassifier, self).__init__()\n",
    "            self.model = model\n",
    "            self.freeze = freeze\n",
    "\n",
    "            if self.freeze:\n",
    "                for p in self.model.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "\n",
    "            self.classifier = nn.Linear(hidden_size , 256)\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "            self.fc_layer1 = nn.Linear(256,128)\n",
    "            self.fc_layer2 = nn.Linear(128,num_classes)\n",
    "            self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "        def forward(self, input_ids, attn_masks):\n",
    "\n",
    "            _,pooler = self.model(input_ids, attn_masks, return_dict=False)\n",
    "            output1 = self.classifier(pooler)\n",
    "            output2 = self.fc_layer1(output1)\n",
    "            output3 = self.fc_layer2(self.dropout(output2))\n",
    "            return output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec350a41-5b3c-406a-af20-3fedbcbb8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "lr = 1e-5\n",
    "batch_size= 64\n",
    "warmup_ratio = 0.06\n",
    "num_epochs = 1\n",
    "log_interval = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed1e1f93-7f89-4ac4-85c5-eb04d821b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 63/160431 [00:36<25:33:49,  1.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(input_ids_batch\u001b[38;5;241m.\u001b[39mto(device), attention_masks_batch\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(y_pred, y_batch)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/molly/lib/python3.8/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/molly/lib/python3.8/site-packages/torch/autograd/__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = codeBERTclassifier(model).to(device)\n",
    "#model=nn.DataParallel(model).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "#random_idx = random.sample(range(len(train)), len(train))\n",
    "#train_idx = random_idx[:int(len(train)*0.9)]\n",
    "#val_idx = random_idx[int(len(train)*0.9):]\n",
    "#train_split = train.iloc[train_idx]\n",
    "#val_split = train.iloc[val_idx]\n",
    "best_models = []\n",
    "\n",
    "#train_dataset = customDataset(train_split,'train')\n",
    "#valid_dataset = customDataset(val_split,'train')\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_dataset = customDataset(train,'train')\n",
    "valid_dataset = customDataset(valid,'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_step = int(total_steps * warmup_ratio)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "valid_loss_min = 0.4\n",
    "valid_acc_max = 0.8\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    batches = 0\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total =0\n",
    "    model.train()\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader): # token_type_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        #y_pred = torch.transpose(model(input_ids_batch.to(device), attention_masks_batch.to(device), token_type_batch.to(device)), 0, 1)[0]\n",
    "        y_pred = model(input_ids_batch.to(device), attention_masks_batch.to(device))\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "        batches += 1\n",
    "        if batches % log_interval == 0:\n",
    "            print(\"Batch Loss: \", total_loss / batches, \"Accuracy: \", correct.float() / total)\n",
    "    scheduler.step()\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    \n",
    "    # VALIDATION #\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(valid_loader):\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(input_ids_batch.to(device), attention_masks_batch.to(device))\n",
    "            valid_loss = F.cross_entropy(y_pred, y_batch)\n",
    "            valid_loss = valid_loss.cpu().detach().numpy()\n",
    "            preds = torch.argmax(y_pred,1)\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "            y_batch = y_batch.cpu().detach().numpy()\n",
    "            batch_acc = (preds==y_batch).mean()\n",
    "            val_loss.append(valid_loss)\n",
    "            val_acc.append(batch_acc)\n",
    "\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_acc = np.mean(val_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch} - valid Loss: {val_loss:.6f} - valid_acc : {val_acc:.6f}')\n",
    "    if valid_acc_max < val_acc:\n",
    "        valid_acc_max = val_acc\n",
    "        best_models.append(model)\n",
    "        torch.save(model.state_dict(), f'Junha Park/DACON_PYNLI/weights/codeBERT-{len(best_models)}.pth') \n",
    "        print('model saved, model val acc : ',val_acc)\n",
    "        print('best models size : ',len(best_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec375e91-d268-4394-abb9-0a482f1c6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean_test(df):\n",
    "    df[\"text_sum\"] = \"[CLS] \"+df[\"code1\"]+\" [SEP] \"+df[\"code2\"]+\" [SEP]\"\n",
    "    df = df[['text_sum', 'code1']]\n",
    "    return df\n",
    "test = text_clean_test(test)\n",
    "test_dataset = customDataset(test, 'test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341be89a-4f10-4eae-9afd-3524c4faf6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179700/179700 [41:30<00:00, 72.16it/s] \n"
     ]
    }
   ],
   "source": [
    "bestm = model\n",
    "bestm.eval()\n",
    "answer = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attention_masks_batch in tqdm(test_dataloader):\n",
    "        y_pred =bestm(input_ids_batch.to(device), attention_masks_batch.to(device)).detach().cpu().numpy()\n",
    "        answer.extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d3e08e3-6cd6-4215-9470-6ec832f1e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for item in answer:\n",
    "    p.append(item.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e90ee77a-11b2-48ea-984c-2004acfe596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss['similar'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6acda92e-3f6b-480a-94fb-b378f3307e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('submission_codeBERTa.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7ee9f-4a99-4cd3-8a4e-f2e05a1b0b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molly",
   "language": "python",
   "name": "molly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
